## 2 请求的发送方法

  ### 2.1requests 模块的基本使用

补充：Python2网络请求

```python
import utllib2
```

##### 1 为什么要重点学习requests模块，而不是urllib

- requests的底层实现就是urllib
- requests在python2 和python3中通用，方法完全一样
- requests简单易用
- Requests能够自动帮助我们解压(gzip压缩的等)响应内容
- request 库已经实现了url**解析转义**， 并实现了**请求参数和url实现了独立，** 

##### 2 requests的作用

作用：发送网络请求，返回响应数据

##### 3 response的常用属性：

- `response.text` 响应体 str类型
- `respones.content` 响应体 bytes类型
- `response.status_code` 响应状态码
- `response.request.headers` 响应对应的请求头
- `response.headers` 响应头
- `response.request.cookies` 响应对应请求的cookie    
- `response.cookies` 响应的cookie（经过了set-cookie动作）

###### 思考：text是response的属性还是方法呢？

- 一般来说名词，往往都是对象的属性，对应的动词是对象的方法

###### 3.1 response.text 和response.content的区别    

- `response.text`    
  - 类型：str   
  - 解码类型： requests模块**自动**根据HTTP 头部对响应的编码作出**有根据的推测**，推测的文本编码   
  - 当解码方式为空的时候， 就会自己寻找解码方式，容易解码成乱码   
  - 如何修改编码方式：`response.encoding=”gbk”`  
  - `response.encoding=”utf-8”`   
- `response.content`
  - 类型：bytes
  - 解码类型： 没有指定
  - 如何修改编码方式：`1 response.content.deocde(“utf8”)`             2  str() 构建

获取网页源码的通用方式：

1. `response.content.decode()`
2. `response.content.decode("GBK")`
3. `response.text  `  注意指定编码方式。

以上三种方法从前往后尝试，能够100%的解决所有网页解码的问题

所以：更推荐使用`response.content.deocde()`的方式获取响应的html页面

##### 4 发送带header的请求

###### 4.1 思考

对比浏览器上百度首页的网页源码和代码中的百度首页的源码，有什么不同？

代码中的百度首页的源码非常少，为什么？

###### 4.2 为什么请求需要带上header？

模拟浏览器，欺骗服务器，获取和浏览器一致的内容

###### 4.3 header的形式：字典

`headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}`

###### 4.4 用法

`requests.get(url, headers=headers)`

##### 5 发送带参数的请求

> 我们在使用百度搜索的时候经常发现url地址中会有一个 `?`，那么该问号后边的就是请求参数，又叫做查询字符串

 5.2 请求参数的形式：字典

`kw = {'wd':'长城'}   `  <u>内部帮助我们转义</u>

###### 5.3 请求参数的用法   

`requests.get(url,params=kw) `   

###### 5.4 关于参数的注意点   

在url地址中， 很多参数是没有用的，比如百度搜索的url地址，其中参数只有一个字段有用，其他的都可以删除 如何确定那些请求参数有用或者没用：  挨个尝试！ 对应的,在后续的爬虫中，越到很多参数的url地址，都可以**尝试删除参数**    

###### 5.5 两种方式：发送带参数的请求    

1 对`https://www.baidu.com/s?wd=python`   发起请求可以使用   `requests.get(url, params=kw)`的方   

2  也可以直接对`https://www.baidu.com/s?wd=python`完整的url直接发送请求，不使用params参数    

**豆瓣电影案例**：

url：加载更多的地址才是真正的地址。

请求方式：get

请求参数：

请求头：

- User_Agent

分析：参数获取时， 选取第二页， 再加载一次， 过滤（search)， 得到第二组参数， 对比。拷贝参数 -- subline  replare （点星 ）：（点星 ）\n    (必须是双引号)

**百度贴吧爬虫**  典型的贴吧爬虫

url ：注意， 点击第二页后的url， 才能根据路由找到对应的获取数据的视图函数

请求方式

请求参数

请求头

面想对象来写

聚焦爬虫

- 1 获取请求列表
  - base_url
  - base_url.format('test', 50) 添空
- 2 发起请求
  - import request 
- 3 从响应中提取数据
- 4 保存数据

**案例：百度翻译**   借助第三方接口完成一些操作。

通过百度翻译接口翻译内容：

1 找百度翻译的接口。

url：    查看preview中有值的那个选项

请求方式：post    

请求参数：    

- GET :使用params   
- POST：使用data    

请求头：   

**出现的第一个问题：翻译的数据出不来**

将请求头补全

<u>补全能获取翻译后的数据， 但是换一个单词就不能获取数据</u>， 原因：sign会随着单词的变化而变化， 

解决sign问题：用户输入的内容发生变化， sign就会发生变化， **js动态生成**的。

解决方式：三种

1 切换移动端（js代码也会不一样）

2  控制浏览器获取内容（性能非常低）（爬虫底线）（5， 6天）

3 js逆向（非常高级， 难度最大）

v2tr：过滤

现在切换移动端， 刷新页面， 翻译不同单词， 此时参数中由于sign消失， 所以不受sign影响， 将对应的headers 和 data 换成移动端数据， 然后重新运行， 可以获取数据， 进行数据的简单筛选。



### 2.2requests 模块的深入使用

##### 1 使用requests发送POST请求

> 思考：哪些地方我们会用到POST请求？

1. 登录注册（ POST 比 GET 更安全）
2. 需要传输大文本内容的时候（ POST 请求对数据长度没有要求）

所以同样的，我们的爬虫也需要在这两个地方回去模拟浏览器发送post请求

###### 1.1 requests发送post请求语法：

- 用法：

  ```
    response = requests.post("http://www.baidu.com/", \
    data = data,headers=headers)
  ```

- data 的形式：字典

######  小结

在模拟登陆等场景，经常需要发送post请求，直接使用`requests.post(url,data)`即可

##### 2 使用代理   proxies

###### 2.1 为什么要使用代理（代理服务器）

1. 让服务器以为不是同一个客户端在请求
2. 防止我们的真实地址被泄露，防止被追究
3. 可以用很多个代理，同时处理多个业务， 防止服务器封

###### 2.3 理解正向代理和反向代理的区别

爬虫一般用正向代理

反向应用场景：负载均衡

- **负载均衡**： <u>1 轮询nginx 轮询其他服务器  a  当nginx 达到上限， 在加新的域名。b 数据库达到上限  主从分离 -- 主数据库负责写入， 从数据库负责读取  c 主数据库达到上限 -- 分表分库</u>    
  - 怎么让四核 八核跑起来
- 正向代理：对于浏览器知道服务器的真实地址，例如VPN
  - 爬虫/客户端知道目标主机
- 反向代理：浏览器不知道服务器的真实地址，例如nginx
  - 爬虫/客户端不知道真正的服务器地址。

https://blog.csdn.net/zhanghanboke/article/details/77488894

###### 2.4 代理的使用

- 用法：

  ```
    requests.get("http://www.baidu.com",  proxies = proxies)
  ```

- proxies的形式：字典

- 例如：

  ```
    proxies = { 
        "http": "http://12.34.56.79:9527", 
        "https": "https://12.34.56.79:9527", 
        }
  ```

###### 2.5 代理IP的分类(快代理)

根据代理ip的匿名程度，代理IP可以分为下面四类：

- 透明代理(Transparent Proxy)：透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁。
- 匿名代理(Anonymous Proxy)：使用匿名代理，别人只能知道你用了代理，无法知道你是谁。
- 高匿代理(Elite proxy或High Anonymity Proxy)：高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。

在使用的使用，毫无疑问使用高匿代理效果最好

从请求使用的协议可以分为：

- http代理
- https代理
- socket代理等

不同分类的代理，在使用的时候需要根据抓取网站的协议来选择

###### 2.6 代理IP使用的注意点

- 反反爬

  使用代理ip是非常必要的一种`反反爬`的方式

  但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫，比如：

  - 一段时间内，检测IP访问的频率，<u>访问太多频繁会屏蔽</u>  （代理解决）

  - 检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽

  - 服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽（像电信等，一般一天就结束屏蔽）

    所以更好的方式在使用代理ip的时候使用随机的方式进行选择使用，不要每次都用一个代理ip

- 代理ip池的更新

  购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。（ip代理池，只返回最好用的代理）

  

  工作中：确实是可以用多线程， 多个服务器， 很快爬完数据。但尽量去隐藏自己身份， 尽量不被发现身份。

  

### 2.3 处理cookie

##### 1 爬虫中的cookie

cookie参数支持cookiejar 和 字典类型

为了能够通过爬虫获取到<u>登录后的页面</u>，或者**是解决通过cookie的反扒**，需要使用request来处理cookie相关的请求 

###### 1.1 爬虫中使用cookie的利弊

1. 带上cookie的好处
   - 能够访问登录后的页面
   - 能够实现部分反反爬
2. 带上cookie的坏处
   - 一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫
   - 那么上面的问题如何解决 ?使用多个账号

###### 1.2 requests处理cookie的方法

使用requests处理cookie有三种方法：

1. cookie字符串放在headers中（拷贝）
2. 把cookie字典放传给请求方法的cookies参数接收（）
3. 使用requests提供的session模块

**githup登录操作**  访问需要**登录**的界面的思路

方式一：

1. 从浏览器中拷贝出已经登录好的Cookie数据
2. 在请求过程中携带这个Cookie数据直接访问页面

方式二：手动传递Cookie

1 直接调用登录请求 获取登陆后的Cookie

what？ ：解决，不全header

2 直接用登录后的Cookie访问登录页面。

方式三：使用session自动传递Cookie

1 利用session取登录， 一旦登录成功， Cookie会自动保存到session对象里面

2 直接利用session去访问

session作用：自动携带cookie

###### 4.2 动手练习：

动手尝试使用session来登录人人网： <http://www.renren.com/PLogin.do> (先不考虑这个url地址从何而来)，请求体的格式：`{"email":"username", "password":"password"}`

##### 思路分析

1. 准备url地址和请求参数
2. 构造session发送post请求
3. 使用session请求个人主页，观察是否请求成功

### 2.4 request模块其他方法

##### 1 requests中cookirJar的处理方法

**使用request获取的resposne对象**，**具有cookies属性**，能够获取对方服务器设置在本地的cookie，**但是如何**使用这些cookie呢？ 

##### 1.1 方法介绍

1. response.cookies是CookieJar类型
2. 使用requests.utils.dict_from_cookiejar，能够实现把cookiejar对象转化为字典

##### 2 requests处理证书错误

出现这个问题的原因是：ssl的证书不安全导致 

##### 2.1 代码中发起请求的效果

那么如果在代码中请求会怎么样呢？

```
import requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url)
```

返回证书错误，如下：

```
ssl.CertificateError ...
```

##### 2.2 解决方案

为了在代码中能够正常的请求，我们修改添加一个参数

```
import requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url,verify=False)
```



##### 3 超时参数的使用



##### 4 retrying模块的使用



### 2.5 js 逆向解析

## 目标

- 知道如何寻找登录的接口
- 知道如何确定js的位置
- 知道如何观察js的执行过程
- 知道js的执行方法

本质：

1 寻找关键点：网络请求的一刹那

2 网络请求之前的代码进行分析

3 比较难的js代码用js2py去执行

##### 1 确定网站的登录接口 

##### 1.1  人人网登录界面代码实现， 从而获取可以实现代码登录， 访问需要登录才能访问的界面。

一个js逆向的思路：

- 首先分析整个爬虫思路的时候， 访问网站登录后才能获取的数据。
- 用户的密码实现了加密， 这个加密是在js发送请求到后端之前实现的， 所以可以对js逆向推导， 可以推导出具体实现加密的js代码 。
- 从请求url角度考虑， 因为加密肯定在发送请求之前，所以只要先找到js中发送请求的位置，加密的位置肯定在发送请求的js代码之前。
- 拷贝url资源路径， Chrome检查 -- 右侧search -- 找到对应的发送登录请求的js代码位置 -- 打断点分析

```javascript 
            t.phoneNum = $(".phonenum").value,
            t.password = $(".password").value,       # t的属性接收用户名  密码 
            e = loginValidate(t),
            t.c1 = c1 || 0,
            e.flag ? ajaxFunc("get", "http://activity.renren.com/livecell/rKey", "", function(e) {     # 发送一个ajax请求， 才有rKey请求
                var n = JSON.parse(e).data;   # 将请求的结果赋值给 n      返回json串    解析json串
                if (0 == n.code) {     # 加密过程
                    t.password = t.password.split("").reverse().join(""),     # 反转
                    setMaxDigits(130);
                    var o = new RSAKeyPair(n.e,"",n.n)       
                      , r = encryptedString(o, t.password);     
                    t.password = r,    
                    t.rKey = n.rkey     
                } else
                    toast("公钥获取失败"),
                    t.rKey = "";
                ajaxFunc("post", "http://activity.renren.com/livecell/ajax/clog", t, function(e) {    # 登录的ajax请求 
                    var e = JSON.parse(e).logInfo;
                    0 == e.code ? location.href = localStorage.getItem("url") || "" : toast(e.msg || "登录出错")
                })
```

模拟请求第一步： 制作 t 变量

二： 发送 请求， 生成n

将核心加密密码拷贝， 生成js_string 借助上下文在Python中运行， 遇到未定义的js函数， 就将改函数对应的js模块拷贝生成文件，然后打开并运行这个文件， 就可以使用对应的js函数了。（这里要注意的是js模块之间的互相调用）



分析：url  请求方式    请求参数   请求头

切换移动端， 也不能实现， 只能用js逆向

3 密码加密， 是在js请求前， 逆推， 

##### 2 确定js的位置

##### 3 观察js的执行过程

##### 4 执行js



### 2.7 chrome 的介绍

##### 1 新建隐身窗口

##### 2 chrome中network的更多功能

###### 2.1 Perserve log

默认情况下，页面发生跳转之后，之前的请求url地址等信息都会消失，勾选perserve log后之前的请求都会被保留  

###### 2.2 filter过滤

在url地址很多的时候，可以在filter中输入部分url地址，对所有的url地址起到一定的过滤效果，具体位置在上面第二幅图中的2的位置 

###### 2.3 观察特定种类的请求

在上面第二幅图中的3的位置，有很多选项，默认是选择的`all`，即会观察到所有种类的请求

很多时候处于自己的目的可以选择`all`右边的其他选项，比如常见的选项：

- XHR:大部分情况表示ajax请求
- JS:js请求
- CSS:css请求

但是很多时候我们并不能保证我们需要的请求是什么类型，特别是我们不清楚一个请求是否为ajax请求的时候，直接选择`all`,从前往后观察即可，其中js，css，图片等不去观察即可

不要被浏览器中的一堆请求吓到了，这些请求中除了js，css，图片的请求外，其他的请求并没有多少个

###### 2.4 其他方法

在前面的课程中我们还介绍了：

- search all file
- 确定js的位置

- js中添加断点















**补充：**

- 数字指纹算法：给一个二进制数据生成一个唯一的32/64位数据， 
- MD5  sha1    sha256



**对称加密算法**：

只有一个密码， 所有密码加解密都适合这个密码

ecb：块状加密：按照一定长度加密

- 缺点是不够安全：相同内容， 加密后的结果相同
- 优点：性能高

cbc 密码联调加密

上一块加密的结果作为下一块要加密内容的密码， 

- 优点：安全性高
- 缺点：性能差， 因为想获取下面的内容， 要将上面的内容都加密

- AES  /AES256/DES 3DS

**非对称算法**：这种算法性能非常非常差， 一般支队密码等关键性数据进行加密

RSA